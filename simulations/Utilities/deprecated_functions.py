
import numpy as np
import os
import pandas as pan
import regex as re
from pathlib import Path
import glob
import shutil
import sys
import argparse
import copy
import warnings

import scipy.stats as stats
import scipy.fftpack as fft
from scipy.signal import find_peaks
from scipy.signal import correlate2d
from scipy.signal import fftconvolve
from scipy.interpolate import CubicSpline
from scipy.interpolate import InterpolatedUnivariateSpline
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from skimage.metrics import structural_similarity as ssim
from sklearn.metrics import mutual_info_score , normalized_mutual_info_score
from sklearn.metrics import adjusted_mutual_info_score
from esda.moran import Moran_BV
from libpysal.weights import Queen, KNN, lat2W
from collections import defaultdict

#from multiprocessing import Pool, cpu_count, Queue, Lock
#import functools


import secrets
rng = np.random.default_rng(secrets.randbits(128))
from glow_up import *









''' # Summary of the function gen_2DCorr_data(...) [MONOTHREAD VERSION OF THE FUNCTION gen_2DCorr_data(...)
IN glow_up.py]
This function gen_NCC_data will generate auto- and cross-correlation data for the files in files.
These files are named in the format: FRAME_T_{T}_a_{a_val}_R_{R}.csv or GAMMA_T_{T}_a_{a_val}_R_{R}.csv.
By iterating over the files in increasing values of R, the column-specific time auto-NCC is generated by auto-correlation of each column in the file,
with a matching column in crossfiles (if provided), with the same R value as the file.
The cross-NCC on the other hand consists of two steps:
1. Pure spatial cross-correlation (at the same time): Cross-correlation of the data in each included column with the data  in another column in the same file.
2. Spatio-temporal cross-correlation of the data in each included column with the data in another column in a different file (crossfiles) with the same R value as the file.
The function will generate the NCC data for each column in the files for each file (replicate).
The auto-NCC (for best shifts with highest peaks) data is stored in a df with the following column structure:
"t0", "t1", "t-delay", "Rmax", "AVG[NCC[{var}]], ...., "NCC[{var}]_R_0", "NCC-Index[{var}]_R_0" ...., "NCC[{var}]_R_{R_max}", "NCC-Index[{var}]_R_{R_max}"
where {var} is one of the species names NOT in exclude_col_labels, Rmax is the maximum R value in the files, t0 is the time of the first frame, t1 is the time of the compared frame, 
and t-delay is the time delay between the two frames.
The cross-NCC (for best shifts with highest peaks) data is stored in a df with the following column structure:
"t0", "t1", "t-delay", "Rmax", "AVG[NCC[{var1}][{var2}]]", ...., "AVG[NCC[{var1}][{varN}]]" , "AVG[NCC[{var2}][{var3}]]", ...., "AVG[NCC[{vari}][{vark}]]" , ...., AVG[NCC[{varN-1}][{varN}]],
"NCC[{var1}][{var2}]_R_0" , "NCC-Index[{var1}][{var2}]_R_0" ...., "NCC[{var1}][{var2}]_R_{R_max}", "NCC-Index[{var1}][{var2}]_R_{R_max}" ...., 
"NCC[{varN-1}][{varN}]_R_{R_max}", "NCC-Index[{varN-1}][{varN}]_R_{R_max}" ...., "NCC[{varN-1}][{varN}]_R_{R_max}", "NCC-Index[{varN-1}][{varN}]_R_{R_max}"
where {var1}, {var2}, ... {varN} are the species names NOT in exclude_col_labels, Rmax is the maximum R value in the files, t0 is the time of the first frame, t1 is the time of the compared frame,
and t-delay is the time delay between the two frames, and i < k (the order of columns in the files).
The function will also generate the mean NCC data across all files for each column in the files, which is stored in AVG[NCC[{var}]] and AVG[NCC[{var1}][{var2}]] for auto- and cross-NCC respectively.

Similarly, computes the Adjusted Mutual Information (AMI) and Mutual Information (MI) for each column in the files for each file (replicate)
in the same way as NCC, and stores them in the same column structure as above.

Also the 2D Pearson correlation coefficient in each case corresponds to the zero-shift NCC, store these values in auto-ZNCC and cross-ZNCC dfs with the same column structure as above.

NOTE: ZNCC is the zero-normalised cross-correlation, so columns in the files are normalised to zero mean and unit variance before calculating the ZNCC.
'''

def gen_2DCorr_data_OLD(files, T, matchT=[], crossfiles=[], pathtodir="", ext="csv", exclude_col_labels= ["a_c", "x", "L", "GAM[P(x; t)]"], 
                    calc_AMI = True, calc_MI = False, bins="scotts", verbose=False):

    # Store unique column names present in files in col_labels. Strip elements of any leading or trailing whitespaces.
    col_labels = []; #L = 0;
    # Iterate over other files in files, and store unique column names in col_labels.
    for file in files:
        if(ext == "csv"):
            df = pan.read_csv(file, header=0)
        else:
            try:
                df = pan.read_table(file, header=0)
            except Exception as e:
                print("Error: Non-standard extension for file " + pathtodir +"/" + file + " with error message: \n" + str(e))
                return None, None, None, None, None, None, None, None
        # Modify column names in df to remove leading and trailing whitespaces.
        df.columns = [col.strip() for col in df.columns]
        # Remove exclude_col_labels from df.
        col_labels.extend([col for col in df.columns if col not in exclude_col_labels])

        if file == files[0]:
            # Get L from the number of rows in the first file.
            L = int(np.sqrt(len(df))) #int(np.sqrt(len(df[df.columns[0]])))
            print(f"Found L = {L}.")
    
    # Remove duplicates  and exclude_col_labels from col_labels.
    col_labels = list(set(col_labels)); col_labels = [col for col in col_labels if col not in exclude_col_labels]

    auto_NCC_cols = ["t0", "t1", "t-delay", "Rmax"] + ["AVG[NCC[" + col + "]" for col in col_labels] + [y + col + "]_R_" + str(i) for col 
                                                                                                        in col_labels for y in ["NCC[", "NCC-Index["] for i in range(0, len(files))]
    cross_NCC_cols = ["t0", "t1", "t-delay", "Rmax"] + ["AVG[NCC[" + col1 + "][" + col2 + "]" for col1 
                                                        in col_labels for col2 in col_labels if col1 != col2] + [y + col1 + "][" + col2 + "]_R_" + str(i) for col1 in col_labels for col2 in col_labels if col1 != col2 for y in ["NCC[", "NCC-Index["]  for i in range(0, len(files))]
    # Create a df with col_labels as column names. This will store the ZNCC data.

    df_auto_NCC = pan.DataFrame(columns= auto_NCC_cols)
    df_cross_NCC = pan.DataFrame(columns= cross_NCC_cols)

    df_auto_ZNCC = pan.DataFrame(columns= auto_NCC_cols) # Stores ZNCC (NCC(0,0)) OR 2D Pearson correlation coefficient for each column in the files.
    df_cross_ZNCC = pan.DataFrame(columns= cross_NCC_cols)


    # auto_NCC_dict and cross_NCC_dict are nested defaultdict structures to store NCC data
    # Structure: auto_NCC_dict[key][column][NCC_key] = value
    auto_NCC_dict = defaultdict(lambda: defaultdict(dict))
    # Structure: cross_NCC_dict[key][column1][column2][NCC_key] = value
    cross_NCC_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))
    auto_ZNCC_dict = defaultdict(lambda: defaultdict(dict))
    cross_ZNCC_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))


    if calc_AMI:
        # Drop all columns containing "NCC-Index" from df_auto_AMI, and replace all occurrences of "NCC" with "AMI".
        auto_AMI_cols = [col.replace("NCC", "AMI") for col in auto_NCC_cols if "NCC-Index" not in col]
        cross_AMI_cols = [col.replace("NCC", "AMI") for col in cross_NCC_cols if "NCC-Index" not in col]
        df_auto_AMI = pan.DataFrame(columns= auto_AMI_cols) ; auto_AMI_dict = defaultdict(lambda: defaultdict(dict))
        df_cross_AMI = pan.DataFrame(columns= cross_AMI_cols); cross_AMI_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))
    else:
        df_auto_AMI = None; df_cross_AMI = None; auto_AMI_dict = None; cross_AMI_dict = None

    if calc_MI:
        # Drop all columns containing "NCC-Index" from df_auto_MI, and replace all occurrences of "NCC" with "MI".
        auto_MI_cols = [col.replace("NCC", "MI") for col in auto_NCC_cols if "NCC-Index" not in col]
        cross_MI_cols = [col.replace("NCC", "MI") for col in cross_NCC_cols if "NCC-Index" not in col]
        df_auto_MI = pan.DataFrame(columns= auto_MI_cols); auto_MI_dict = defaultdict(lambda: defaultdict(dict))
        df_cross_MI = pan.DataFrame(columns= cross_MI_cols); cross_MI_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))
    else:
        df_auto_MI = None; df_cross_MI = None; auto_MI_dict = None; cross_MI_dict = None

    # matchT is a sorted (ascending order) list of t values of crossfiles <= T.
    for tval in reversed(matchT):

        key = (T, tval, T - float(tval), len(files)) # Common key for all files.
        print(f"Generating 2D correlation data for T0 = {T}, T1 = {tval}, delay = {T - float(tval)}, L = {L} ....")
        # Iterate over all files in files, sorted in ascending order of R.
        for file in sorted(files, key= lambda x: int(re.sub("R_", "", re.findall(r'R_[\d]+', x)[0]))):
            # Read the file.
            Rstr = re.findall(r'R_[\d]+', file)[0]
            Lfile = int(re.search(r'L_[\d]+', file).group(0).split("_")[1]) if re.search(r'L_[\d]+', file) else None 
            filetype = re.search(r'FRAME|GAMMA', file).group(0)
            #R = int(re.search(r'R_[\d]+', file).group(0).split("_")[1])
            if Lfile != L:
                print(f"Error: File {file} has different L value than expected. Expected L = {L}, but got L = {Lfile}. Skipping file.")
                continue

            if(ext == "csv"):
                df = pan.read_csv(file, header=0)
            else:
                try:
                    df = pan.read_table(file, header=0)
                except Exception as e:
                    print("Error: Non-standard extension for file " + pathtodir +"/" + file + " with error message: \n" + str(e))
                    return None, None, None, None, None, None, None, None

            # Modify column names in df to remove leading and trailing whitespaces.
            df.columns = [col.strip() for col in df.columns]
            # Remove exclude_col_labels from df.
            df = df[[col for col in df.columns if col not in exclude_col_labels]]

            df_Znorm = gen_zer0mean_normalised_df(df) # Generate 0-normalised columns for df.
                
            #Get crossfiles with the same T value as tval and R value as Rstr.
            crossfile_tval = [x  for x in crossfiles
                                if ((m := re.findall(r'T_(\d+)', x)) and m[0] == str(tval)) 
                                or ((m := re.findall(r'T_([\d]+\.[\d]+)', x)) and m[0] == str(tval))]

            #if( re.findall(r'T_[\d]+', x)[0] and re.findall(r'T_[\d]+', x)[0] == "T_" + str(tval))
            #                  else (re.findall(r'T_[\d]*[.][\d]+', x)[0] and re.findall(r'T_[\d]*[.][\d]+', x)[0] == "T_" + str(tval))
            
            crossfile_tval = [x for x in crossfile_tval  if re.search(r'R_[\d]+', x).group(0) == Rstr and re.search(r'FRAME|GAMMA', x).group(0) == filetype]
            # If no or more than 1 are found, skip the column.
            if len(crossfile_tval) != 1:
                print(f"Warning: {len(crossfile_tval)} crossfiles found for T0={T}, T1={tval}, R={Rstr}, skipping...")
                continue
            # Read the crossfile and zero-normalise all columns in the crossfile.
            if(ext == "csv"):
                df_cross = pan.read_csv(crossfile_tval[0], header=0)
            else:
                try:
                    df_cross = pan.read_table(crossfile_tval[0], header=0)
                except Exception as e:
                    print("Error: Non-standard extension for file " + pathtodir +"/" + crossfile_tval[0] + " with error message: \n" + str(e))
                    return None, None, None, None, None, None, None, None
            # Modify column names in df_cross to remove leading and trailing whitespaces.
            df_cross.columns = [col.strip() for col in df_cross.columns]
            # Remove exclude_col_labels from df_cross.
            df_cross = df_cross[[col for col in df_cross.columns if col not in exclude_col_labels]]
            df_cross_Znorm = gen_zer0mean_normalised_df(df_cross) # Generate 0-normalised columns for df_cross.

            print(f"Crossing FOCUS file {os.path.basename(file)} with CROSSfile {os.path.basename(crossfile_tval[0])} at R = {Rstr} ...")

            

            # First auto-NCC and auto-ZNCC: Iterate over all df.columns and over T values in matchT, and calculate the NCC for each column in df.
            for col in df_Znorm.columns:
                # Calculate the NCC for each column in df with the same column in df_cross.
                if col not in df_cross_Znorm.columns:
                    print(f"Warning: Column {col} not found in crossfile {os.path.basename(crossfile_tval[0])}")
                    continue

                # Also check if the column is entirely zero or flat. If so, skip the column.
                if all(df_Znorm[col] == 0) or all(df_cross_Znorm[col] == 0):
                    print(f"Warning: Entirely zero column {col} in FOCUS file {os.path.basename(file)}" + 
                        f"or CROSSfile {os.path.basename(crossfile_tval[0])} for T0 = {T}, T1 = {tval} ... Skipping column.")
                    continue

                print(f"Calculating 2D AUTO Correlation for column {col} ...")
                
                if(calc_AMI):
                    # Calculate AMI for each column in df with the same column in df_cross.
                    AMI = compute_adjusted_mutual_information(df[col].values, df_cross[col].values, bins=bins)
                    AMI_key = f"AMI[{col}]_{Rstr}"; auto_AMI_dict[key][col][AMI_key] = AMI
                if(calc_MI):
                    # Calculate MI for each column in df with the same column in df_cross.
                    MI = compute_mutual_information(df[col].values, df_cross[col].values, bins=bins)
                    MI_key = f"MI[{col}]_{Rstr}"; auto_MI_dict[key][col][MI_key] = MI

                x = df_Znorm[col].values.reshape(int(np.sqrt(len(df_Znorm))), -1)
                y = df_cross_Znorm[col].values.reshape(int(np.sqrt(len(df_cross_Znorm))), -1)

                NCC_key = f"NCC[{col}]_{Rstr}"; NCC_index_key = f"NCC-Index[{col}]_{Rstr}"
                ZNCC_key = f"ZNCC[{col}]_{Rstr}"

                ncc_map, zncc, peak, peak_idx = compute_NCC_2DFFT(x, y)
                if zncc is None:
                    print(f"Warning: Skipping column {col} due to flat or zero data in correlation.")
                    continue
                # Store the NCC data in df_auto_NCC, and the ZNCC data in df_auto_ZNCC.
                
                auto_NCC_dict[key][col][NCC_key] = peak
                auto_NCC_dict[key][col][NCC_index_key] = int(peak_idx[0] * L + peak_idx[1]) # Convert to 1D index.
                auto_ZNCC_dict[key][col][ZNCC_key] = zncc

                
            
            # Next cross-NCC and cross-ZNCC.
            for i, col1 in enumerate(df_Znorm.columns):
                
                if all(df_Znorm[col1] == 0):
                    print(f"Warning: Entirely zero column {col1} in FOCUS file {os.path.basename(file)} for T0 = {T} ... Skipping column.")
                    continue

                for j, col2 in enumerate(df_cross_Znorm.columns):
                    if col1 == col2:
                        continue

                    # Also check if the column is entirely zero or flat. If so, skip the column.
                    if all(df_cross_Znorm[col2] == 0):
                        print(f"Warning: Entirely zero COL: {col2} in CROSSfile {os.path.basename(crossfile_tval[0])} for T0 = {T}, T1 = {tval} ... Skipping column.")
                        continue

                    print(f"Calculating 2D CROSS Correlation for columns {col1} VS {col2} ...")

                    x = df_Znorm[col1].values.reshape(int(np.sqrt(len(df_Znorm))), -1)
                    y = df_cross_Znorm[col2].values.reshape(int(np.sqrt(len(df_cross_Znorm))), -1)
                    ncc_map, zncc, peak, peak_idx = compute_NCC_2DFFT(x, y)
                    NCC_index_key = f"NCC-Index[{col1}][{col2}]_{Rstr}"; NCC_key = f"NCC[{col1}][{col2}]_{Rstr}"
                    ZNCC_key = f"ZNCC[{col1}][{col2}]_{Rstr}"

                    if zncc is None:
                        print(f"Warning: Skipping columns {col1}, {col2} due to flat or zero data.")
                        continue

                    cross_NCC_dict[key][col1][col2][NCC_key] = peak
                    cross_NCC_dict[key][col1][col2][NCC_index_key] = int(peak_idx[0] * L + peak_idx[1]) 
                    # Convert to 1D index.
                    cross_ZNCC_dict[key][col1][col2][ZNCC_key] = zncc

                    if(calc_AMI):
                        # Calculate AMI for each column in df with the same column in df_cross.
                        AMI = compute_adjusted_mutual_information(df[col1].values, df_cross[col2].values, bins=bins)
                        AMI_key = f"AMI[{col1}][{col2}]_{Rstr}"; cross_AMI_dict[key][col1][col2][AMI_key] = AMI
                    if(calc_MI):
                        # Calculate MI for each column in df with the same column in df_cross.
                        MI = compute_mutual_information(df[col1].values, df_cross[col2].values, bins=bins)
                        MI_key = f"MI[{col1}][{col2}]_{Rstr}"; cross_MI_dict[key][col1][col2][MI_key] = MI

    # Done with all files in files.
    

    # Construct the dataframes for auto-NCC and cross-NCC.
    # Iterate over all keys in auto_NCC_dict and cross_NCC_dict, and store the data in df_auto_NCC and df_cross_NCC.
    for key, col_data in auto_NCC_dict.items():
        # Recall auto_NCC_dict is a nested dictionary with the structure:
        # auto_NCC_dict[key][col][NCC_key] = peak, auto_NCC_dict[key][col][NCC_index_key] = int(peak_idx[0] * L + peak_idx[1])
        # auto_NCC_dict[key][col][ZNCC_key] = zncc
        row = {"t0": key[0], "t1": key[1], "t-delay": key[2], "Rmax": key[3]}
        # Iterating over the species (variables) (col) with each rep_col_data corresponding to all NCC[{col}]_R_* data.
        for var, rep_var_data in col_data.items():
            # Get the NCC and ZNCC data from rep_data by iterating over the keys.
            NCC_rep_vals = [val for repkey, val in rep_var_data.items() if repkey.startswith("NCC[")]
            NCC_index_rep_vals = [val for repkey, val in rep_var_data.items() if repkey.startswith("NCC-Index[")]
            #Now get average across all replicates for NCC and generate row for df_auto_NCC.
            if len(NCC_rep_vals) > 0:
                row[f"AVG[NCC[{col}]]"] = np.mean(NCC_rep_vals)
            row.update(rep_var_data)    # Updating all the replicate data for a specific column (species) in the row.
        # Append the row to df_auto_NCC.
        df_auto_NCC = pan.concat([df_auto_NCC, pan.DataFrame([row])], ignore_index=True)

    # Now for auto-ZNCC.
    for key, col_data in auto_ZNCC_dict.items():
        # Recall auto_ZNCC_dict is a nested dictionary with the structure:
        # auto_ZNCC_dict[key][col][ZNCC_key] = zncc
        row = {"t0": key[0], "t1": key[1], "t-delay": key[2], "Rmax": key[3]}
        # Iterating over the species (variables) (col) with each rep_col_data corresponding to all ZNCC[{col}]_R_* data.
        for var, rep_var_data in col_data.items():
            # Get the ZNCC data from rep_data by iterating over the keys.
            ZNCC_rep_vals = [val for repkey, val in rep_var_data.items() if repkey.startswith("ZNCC[")]
            #Now get average across all replicates for ZNCC and generate row for df_auto_ZNCC.
            if len(ZNCC_rep_vals) > 0:
                row[f"AVG[ZNCC[{var}]]"] = np.mean(ZNCC_rep_vals)
            row.update(rep_var_data)    # Updating all the replicate data for a specific column (species) in the row.
        # Append the row to df_auto_ZNCC.
        df_auto_ZNCC = pan.concat([df_auto_ZNCC, pan.DataFrame([row])], ignore_index=True)

    # Construct the dataframes for cross-NCC.
    # Iterate over all keys in cross_NCC_dict and store the data in df_cross_NCC.
    for key, col_data in cross_NCC_dict.items():
        # Recall cross_NCC_dict is a nested dictionary with the structure:
        # cross_NCC_dict[key][col1][col2][NCC_key] = peak, cross_NCC_dict[key][col1][col2][NCC_index_key] = int(peak_idx[0] * L + peak_idx[1])
        # cross_NCC_dict[key][col1][col2][ZNCC_key] = zncc
        row = {"t0": key[0], "t1": key[1], "t-delay": key[2], "Rmax": key[3]}
        # Iterating over the species (variables) (col) with each rep_col_data corresponding to all NCC[{col}]_R_* data.
        for var1, col_data_2 in col_data.items():
            for var2, rep_var1var2_data in col_data_2.items():
                # Get the NCC and ZNCC data from rep_data by iterating over the keys.
                NCC_rep_vals = [val for repkey, val in rep_var1var2_data.items() if repkey.startswith("NCC[")]
                NCC_index_rep_vals = [val for repkey, val in rep_var1var2_data.items() if repkey.startswith("NCC-Index[")]
                #Now get average across all replicates for NCC and generate row for df_cross_NCC.
                if len(NCC_rep_vals) > 0:
                    row[f"AVG[NCC[{var1}][{var2}]]"] = np.mean(NCC_rep_vals)
                row.update(rep_var1var2_data)    # Updating all the replicate data for a specific column (species) in the row.
        # Append the row to df_cross_NCC.
        df_cross_NCC = pan.concat([df_cross_NCC, pan.DataFrame([row])], ignore_index=True)

    # Now for cross-ZNCC.
    for key, col_data in cross_ZNCC_dict.items():
        # Recall cross_ZNCC_dict is a nested dictionary with the structure:
        # cross_ZNCC_dict[key][col1][col2][ZNCC_key] = zncc
        row = {"t0": key[0], "t1": key[1], "t-delay": key[2], "Rmax": key[3]}
        # Iterating over the species (variables) (col) with each rep_col_data corresponding to all ZNCC[{col}]_R_* data.
        for var1, col_data_2 in col_data.items():
            for var2, rep_var1var2_data in col_data_2.items():
                # Get the ZNCC data from rep_data by iterating over the keys.
                ZNCC_rep_vals = [val for repkey, val in rep_var1var2_data.items() if repkey.startswith("ZNCC[")]
                #Now get average across all replicates for ZNCC and generate row for df_cross_ZNCC.
                if len(ZNCC_rep_vals) > 0:
                    row[f"AVG[ZNCC[{var1}][{var2}]]"] = np.mean(ZNCC_rep_vals)
                row.update(rep_var1var2_data)    # Updating all the replicate data for a specific column (species) in the row.
        # Append the row to df_cross_ZNCC.
        df_cross_ZNCC = pan.concat([df_cross_ZNCC, pan.DataFrame([row])], ignore_index=True)

    # Now for AMI and MI data.
    if( calc_AMI and auto_AMI_dict is not None):
        for key, col_data in auto_AMI_dict.items():
            # Recall auto_AMI_dict is a nested dictionary with the structure:
            # auto_AMI_dict[key][col][AMI_key] = AMI
            row = {"t0": key[0], "t1": key[1], "t-delay": key[2], "Rmax": key[3]}
            # Iterating over the species (variables) (col) with each rep_col_data corresponding to all AMI[{col}]_R_* data.
            for var, rep_var_data in col_data.items():
                # Get the AMI data from rep_data by iterating over the keys.
                AMI_rep_vals = [val for repkey, val in rep_var_data.items() if repkey.startswith("AMI[")]
                #Now get average across all replicates for AMI and generate row for df_auto_AMI.
                if len(AMI_rep_vals) > 0:
                    row[f"AVG[AMI[{var}]]"] = np.mean(AMI_rep_vals)
                row.update(rep_var_data)    # Updating all the replicate data for a specific column (species) in the row.
            # Append the row to df_auto_AMI.
            df_auto_AMI = pan.concat([df_auto_AMI, pan.DataFrame([row])], ignore_index=True)

    if( calc_AMI and cross_AMI_dict is not None):
        for key, col_data in cross_AMI_dict.items():
            # Recall cross_AMI_dict is a nested dictionary with the structure:
            # cross_AMI_dict[key][col1][col2][AMI_key] = AMI
            row = {"t0": key[0], "t1": key[1], "t-delay": key[2], "Rmax": key[3]}
            # Iterating over the species (variables) (col) with each rep_col_data corresponding to all AMI[{col}]_R_* data.
            for var1, col_data_2 in col_data.items():
                for var2, rep_var1var2_data in col_data_2.items():
                    # Get the AMI data from rep_data by iterating over the keys.
                    AMI_rep_vals = [val for repkey, val in rep_var1var2_data.items() if repkey.startswith("AMI[")]
                    #Now get average across all replicates for AMI and generate row for df_cross_AMI.
                    if len(AMI_rep_vals) > 0:
                        row[f"AVG[AMI[{var1}][{var2}]]"] = np.mean(AMI_rep_vals)
                    row.update(rep_var1var2_data)    # Updating all the replicate data for a specific column (species) in the row.
            # Append the row to df_cross_AMI.
            df_cross_AMI = pan.concat([df_cross_AMI, pan.DataFrame([row])], ignore_index=True)

    if( calc_MI and auto_MI_dict is not None):
        for key, col_data in auto_MI_dict.items():
            # Recall auto_MI_dict is a nested dictionary with the structure:
            # auto_MI_dict[key][col][MI_key] = MI
            row = {"t0": key[0], "t1": key[1], "t-delay": key[2], "Rmax": key[3]}
            # Iterating over the species (variables) (col) with each rep_col_data corresponding to all MI[{col}]_R_* data.
            for var, rep_var_data in col_data.items():
                # Get the MI data from rep_data by iterating over the keys.
                MI_rep_vals = [val for repkey, val in rep_var_data.items() if repkey.startswith("MI[")]
                #Now get average across all replicates for MI and generate row for df_auto_MI.
                if len(MI_rep_vals) > 0:
                    row[f"AVG[MI[{var}]]"] = np.mean(MI_rep_vals)
                row.update(rep_var_data)    # Updating all the replicate data for a specific column (species) in the row.
            # Append the row to df_auto_MI.
            df_auto_MI = pan.concat([df_auto_MI, pan.DataFrame([row])], ignore_index=True)

    if( calc_MI and cross_MI_dict is not None):
        for key, col_data in cross_MI_dict.items():
            # Recall cross_MI_dict is a nested dictionary with the structure:
            # cross_MI_dict[key][col1][col2][MI_key] = MI
            row = {"t0": key[0], "t1": key[1], "t-delay": key[2], "Rmax": key[3]}
            # Iterating over the species (variables) (col) with each rep_col_data corresponding to all MI[{col}]_R_* data.
            for var1, col_data_2 in col_data.items():
                for var2, rep_var1var2_data in col_data_2.items():
                    # Get the MI data from rep_data by iterating over the keys.
                    MI_rep_vals = [val for repkey, val in rep_var1var2_data.items() if repkey.startswith("MI[")]
                    #Now get average across all replicates for MI and generate row for df_cross_MI.
                    if len(MI_rep_vals) > 0:
                        row[f"AVG[MI[{var1}][{var2}]]"] = np.mean(MI_rep_vals)
                    row.update(rep_var1var2_data)    # Updating all the replicate data for a specific column (species) in the row.
            # Append the row to df_cross_MI.
            df_cross_MI = pan.concat([df_cross_MI, pan.DataFrame([row])], ignore_index=True)

    # Drop all empty columns from df_auto_NCC, df_cross_NCC, df_auto_ZNCC, df_cross_ZNCC, df_auto_AMI, df_cross_AMI, df_auto_MI, df_cross_MI.
    # This is done to avoid any errors while saving the dataframes to csv files.
    df_auto_NCC.dropna(axis=1, how='all', inplace=True)
    df_cross_NCC.dropna(axis=1, how='all', inplace=True)
    df_auto_ZNCC.dropna(axis=1, how='all', inplace=True)
    df_cross_ZNCC.dropna(axis=1, how='all', inplace=True)
    if df_auto_AMI is not None:
        df_auto_AMI.dropna(axis=1, how='all', inplace=True)
    if df_cross_AMI is not None:
        df_cross_AMI.dropna(axis=1, how='all', inplace=True)
    if df_auto_MI is not None:
        df_auto_MI.dropna(axis=1, how='all', inplace=True)
    if df_cross_MI is not None:
        df_cross_MI.dropna(axis=1, how='all', inplace=True)

    # Return dataframes for auto-NCC, cross-NCC, auto-ZNCC, cross-ZNCC, auto-AMI, cross-AMI, auto-MI, cross-MI.
    # If any of the dataframes are empty, return None for that dataframe.
    df_auto_NCC = None if df_auto_NCC.empty else df_auto_NCC
    df_cross_NCC = None if df_cross_NCC.empty else df_cross_NCC
    df_auto_ZNCC = None if df_auto_ZNCC.empty else df_auto_ZNCC
    df_cross_ZNCC = None if df_cross_ZNCC.empty else df_cross_ZNCC

    return df_auto_NCC, df_cross_NCC, df_auto_ZNCC, df_cross_ZNCC, df_auto_AMI, df_cross_AMI, df_auto_MI, df_cross_MI
    



import numpy as np
import pandas as pan
import cupy as cp
from cupyx.scipy.signal import fftconvolve as gpu_fftconvolve
from collections import defaultdict
import re
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from functools import partial

# GPU-accelerated compute_NCC_2DFFT
def compute_NCC_2DFFT_GPU(x, y, zero_norm=True):
    """GPU-accelerated version of compute_NCC_2DFFT using CuPy"""
    # Convert to GPU arrays
    x_gpu = cp.asarray(x)
    y_gpu = cp.asarray(y)
    
    std_x = cp.std(x_gpu)
    std_y = cp.std(y_gpu)
    if std_x < 1e-8 or std_y < 1e-8:
        return None, None, None, None
    
    if zero_norm:
        x_gpu = (x_gpu - cp.mean(x_gpu)) / std_x
        y_gpu = (y_gpu - cp.mean(y_gpu)) / std_y
    
    # NCC computed using GPU FFT convolution
    corr_gpu = gpu_fftconvolve(x_gpu, y_gpu[::-1, ::-1], mode='full') / (x_gpu.shape[0] * x_gpu.shape[1])
    max_idx = cp.unravel_index(cp.argmax(corr_gpu), corr_gpu.shape)
    zero_shift_idx = (y_gpu.shape[0]-1, y_gpu.shape[1]-1)
    
    # Convert back to CPU for return values
    corr = cp.asnumpy(corr_gpu)
    zncc = cp.asnumpy(corr_gpu[zero_shift_idx])
    peak = cp.asnumpy(cp.max(corr_gpu))
    max_idx = (cp.asnumpy(max_idx[0]), cp.asnumpy(max_idx[1]))
    
    return corr, zncc, peak, max_idx

# GPU-accelerated mutual information computation
def compute_adjusted_mutual_information_GPU(x, y, bins='scotts'):
    """GPU-accelerated AMI computation using CuPy"""
    try:
        import cuml
        from cuml.metrics import adjusted_mutual_info_score
        
        # Convert to GPU arrays
        x_gpu = cp.asarray(x)
        y_gpu = cp.asarray(y)
        
        # Compute histograms on GPU for binning
        if bins == 'scotts':
            n = len(x)
            bins = int(np.ceil(2 * n**(1/3)))
        
        # Discretize the data
        x_binned = cp.digitize(x_gpu, cp.linspace(cp.min(x_gpu), cp.max(x_gpu), bins))
        y_binned = cp.digitize(y_gpu, cp.linspace(cp.min(y_gpu), cp.max(y_gpu), bins))
        
        # Use cuML's AMI if available
        ami = adjusted_mutual_info_score(cp.asnumpy(x_binned), cp.asnumpy(y_binned))
        return ami
        
    except ImportError:
        # Fallback to CPU implementation if cuML not available
        return compute_adjusted_mutual_information(x, y, bins=bins)

def compute_mutual_information_GPU(x, y, bins='scotts'):
    """GPU-accelerated MI computation"""
    try:
        import cuml
        from cuml.metrics import mutual_info_score
        
        x_gpu = cp.asarray(x)
        y_gpu = cp.asarray(y)
        
        if bins == 'scotts':
            n = len(x)
            bins = int(np.ceil(2 * n**(1/3)))
        
        x_binned = cp.digitize(x_gpu, cp.linspace(cp.min(x_gpu), cp.max(x_gpu), bins))
        y_binned = cp.digitize(y_gpu, cp.linspace(cp.min(y_gpu), cp.max(y_gpu), bins))
        
        mi = mutual_info_score(cp.asnumpy(x_binned), cp.asnumpy(y_binned))
        return mi
        
    except ImportError:
        return compute_mutual_information(x, y, bins=bins)

# Batch processing function for GPU
def process_correlation_batch_GPU(file_batch, crossfile_batch, key, col_labels, L, exclude_col_labels, 
                                  calc_AMI, calc_MI, bins, ext="csv"):
    """Process a batch of files on GPU"""
    batch_auto_NCC = defaultdict(lambda: defaultdict(dict))
    batch_cross_NCC = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))
    batch_auto_ZNCC = defaultdict(lambda: defaultdict(dict))
    batch_cross_ZNCC = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))
    batch_auto_AMI = defaultdict(lambda: defaultdict(dict)) if calc_AMI else None
    batch_cross_AMI = defaultdict(lambda: defaultdict(lambda: defaultdict(dict))) if calc_AMI else None
    batch_auto_MI = defaultdict(lambda: defaultdict(dict)) if calc_MI else None
    batch_cross_MI = defaultdict(lambda: defaultdict(lambda: defaultdict(dict))) if calc_MI else None
    
    # Pre-allocate GPU memory for batch processing
    gpu_arrays = {}
    
    for file_info, crossfile_info in zip(file_batch, crossfile_batch):
        file, Rstr = file_info
        crossfile, _ = crossfile_info
        
        # Load and preprocess data
        if ext == "csv":
            df = pan.read_csv(file, header=0)
            df_cross = pan.read_csv(crossfile, header=0)
        else:
            df = pan.read_table(file, header=0)
            df_cross = pan.read_table(crossfile, header=0)
        
        # Clean column names
        df.columns = [col.strip() for col in df.columns]
        df_cross.columns = [col.strip() for col in df_cross.columns]
        
        # Remove excluded columns
        df = df[[col for col in df.columns if col not in exclude_col_labels]]
        df_cross = df_cross[[col for col in df_cross.columns if col not in exclude_col_labels]]
        
        # Generate zero-normalized dataframes
        df_Znorm = gen_zer0mean_normalised_df(df)
        df_cross_Znorm = gen_zer0mean_normalised_df(df_cross)
        
        # Convert to GPU arrays and cache
        for col in df_Znorm.columns:
            if col in df_cross_Znorm.columns:
                gpu_arrays[f"{file}_{col}"] = cp.asarray(df_Znorm[col].values.reshape(L, -1))
                gpu_arrays[f"{crossfile}_{col}"] = cp.asarray(df_cross_Znorm[col].values.reshape(L, -1))
        
        # Process auto-correlations
        for col in df_Znorm.columns:
            if col not in df_cross_Znorm.columns:
                continue
            
            if all(df_Znorm[col] == 0) or all(df_cross_Znorm[col] == 0):
                continue
            
            x_gpu = gpu_arrays[f"{file}_{col}"]
            y_gpu = gpu_arrays[f"{crossfile}_{col}"]
            
            # Compute correlations on GPU
            ncc_map, zncc, peak, peak_idx = compute_NCC_2DFFT_GPU(
                cp.asnumpy(x_gpu), cp.asnumpy(y_gpu)
            )
            
            if zncc is not None:
                NCC_key = f"NCC[{col}]_{Rstr}"
                NCC_index_key = f"NCC-Index[{col}]_{Rstr}"
                ZNCC_key = f"ZNCC[{col}]_{Rstr}"
                
                batch_auto_NCC[key][col][NCC_key] = peak
                batch_auto_NCC[key][col][NCC_index_key] = int(peak_idx[0] * L + peak_idx[1])
                batch_auto_ZNCC[key][col][ZNCC_key] = zncc
                
                if calc_AMI:
                    AMI = compute_adjusted_mutual_information_GPU(df[col].values, df_cross[col].values, bins=bins)
                    AMI_key = f"AMI[{col}]_{Rstr}"
                    batch_auto_AMI[key][col][AMI_key] = AMI
                
                if calc_MI:
                    MI = compute_mutual_information_GPU(df[col].values, df_cross[col].values, bins=bins)
                    MI_key = f"MI[{col}]_{Rstr}"
                    batch_auto_MI[key][col][MI_key] = MI
        
        # Process cross-correlations
        for col1 in df_Znorm.columns:
            if all(df_Znorm[col1] == 0):
                continue
                
            for col2 in df_cross_Znorm.columns:
                if col1 == col2 or all(df_cross_Znorm[col2] == 0):
                    continue
                
                x_gpu = gpu_arrays[f"{file}_{col1}"]
                y_gpu = gpu_arrays[f"{crossfile}_{col2}"]
                
                ncc_map, zncc, peak, peak_idx = compute_NCC_2DFFT_GPU(
                    cp.asnumpy(x_gpu), cp.asnumpy(y_gpu)
                )
                
                if zncc is not None:
                    NCC_key = f"NCC[{col1}][{col2}]_{Rstr}"
                    NCC_index_key = f"NCC-Index[{col1}][{col2}]_{Rstr}"
                    ZNCC_key = f"ZNCC[{col1}][{col2}]_{Rstr}"
                    
                    batch_cross_NCC[key][col1][col2][NCC_key] = peak
                    batch_cross_NCC[key][col1][col2][NCC_index_key] = int(peak_idx[0] * L + peak_idx[1])
                    batch_cross_ZNCC[key][col1][col2][ZNCC_key] = zncc
                    
                    if calc_AMI:
                        AMI = compute_adjusted_mutual_information_GPU(df[col1].values, df_cross[col2].values, bins=bins)
                        AMI_key = f"AMI[{col1}][{col2}]_{Rstr}"
                        batch_cross_AMI[key][col1][col2][AMI_key] = AMI
                    
                    if calc_MI:
                        MI = compute_mutual_information_GPU(df[col1].values, df_cross[col2].values, bins=bins)
                        MI_key = f"MI[{col1}][{col2}]_{Rstr}"
                        batch_cross_MI[key][col1][col2][MI_key] = MI
    
    # Clear GPU memory
    for array in gpu_arrays.values():
        del array
    cp.get_default_memory_pool().free_all_blocks()
    
    return (batch_auto_NCC, batch_cross_NCC, batch_auto_ZNCC, batch_cross_ZNCC,
            batch_auto_AMI, batch_cross_AMI, batch_auto_MI, batch_cross_MI)

def gen_2DCorr_data_GPU_PARALLEL(files, T, matchT=[], crossfiles=[], pathtodir="", ext="csv", 
                                exclude_col_labels=["a_c", "x", "L", "GAM[P(x; t)]"], 
                                calc_AMI=True, calc_MI=False, bins="scotts", verbose=False,
                                batch_size=4, max_workers=2):
    """
    GPU-parallelized version of gen_2DCorr_data with multi-GPU support and batch processing
    
    Parameters:
    -----------
    batch_size : int
        Number of files to process in each GPU batch
    max_workers : int
        Number of parallel workers (should match number of available GPUs)
    """
    
    # Initialize data collection structures (same as original)
    col_labels = []
    for file in files:
        if ext == "csv":
            df = pan.read_csv(file, header=0)
        else:
            df = pan.read_table(file, header=0)
        
        df.columns = [col.strip() for col in df.columns]
        col_labels.extend([col for col in df.columns if col not in exclude_col_labels])
        
        if file == files[0]:
            L = int(np.sqrt(len(df)))
            print(f"Found L = {L}.")
    
    col_labels = list(set(col_labels))
    col_labels = [col for col in col_labels if col not in exclude_col_labels]
    
    # Initialize result containers
    auto_NCC_dict = defaultdict(lambda: defaultdict(dict))
    cross_NCC_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))
    auto_ZNCC_dict = defaultdict(lambda: defaultdict(dict))
    cross_ZNCC_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))
    auto_AMI_dict = defaultdict(lambda: defaultdict(dict)) if calc_AMI else None
    cross_AMI_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict))) if calc_AMI else None
    auto_MI_dict = defaultdict(lambda: defaultdict(dict)) if calc_MI else None
    cross_MI_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(dict))) if calc_MI else None
    
    # Process each time value with GPU parallelization
    for tval in reversed(matchT):
        key = (T, tval, T - float(tval), len(files))
        print(f"Generating 2D correlation data for T0 = {T}, T1 = {tval}, delay = {T - float(tval)}, L = {L} ....")
        
        # Prepare file batches
        sorted_files = sorted(files, key=lambda x: int(re.sub("R_", "", re.findall(r'R_[\d]+', x)[0])))
        
        file_info_list = []
        crossfile_info_list = []
        
        for file in sorted_files:
            Rstr = re.findall(r'R_[\d]+', file)[0]
            filetype = re.search(r'FRAME|GAMMA', file).group(0)
            
            # Find matching crossfile
            crossfile_tval = [x for x in crossfiles
                            if ((m := re.findall(r'T_(\d+)', x)) and m[0] == str(tval)) 
                            or ((m := re.findall(r'T_([\d]+\.[\d]+)', x)) and m[0] == str(tval))]
            
            crossfile_tval = [x for x in crossfile_tval 
                            if re.search(r'R_[\d]+', x).group(0) == Rstr 
                            and re.search(r'FRAME|GAMMA', x).group(0) == filetype]
            
            if len(crossfile_tval) == 1:
                file_info_list.append((file, Rstr))
                crossfile_info_list.append((crossfile_tval[0], Rstr))
        
        # Create batches
        file_batches = [file_info_list[i:i+batch_size] for i in range(0, len(file_info_list), batch_size)]
        crossfile_batches = [crossfile_info_list[i:i+batch_size] for i in range(0, len(crossfile_info_list), batch_size)]
        
        # Process batches in parallel across multiple GPUs
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Create partial function with common parameters
            process_func = partial(
                process_correlation_batch_GPU,
                key=key, col_labels=col_labels, L=L, exclude_col_labels=exclude_col_labels,
                calc_AMI=calc_AMI, calc_MI=calc_MI, bins=bins, ext=ext
            )
            
            # Submit all batches
            future_to_batch = {
                executor.submit(process_func, file_batch, crossfile_batch): i
                for i, (file_batch, crossfile_batch) in enumerate(zip(file_batches, crossfile_batches))
            }
            
            # Collect results
            for future in as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    (batch_auto_NCC, batch_cross_NCC, batch_auto_ZNCC, batch_cross_ZNCC,
                     batch_auto_AMI, batch_cross_AMI, batch_auto_MI, batch_cross_MI) = future.result()
                    
                    # Merge batch results into main dictionaries
                    for k, v in batch_auto_NCC.items():
                        for col, data in v.items():
                            auto_NCC_dict[k][col].update(data)
                    
                    for k, v in batch_cross_NCC.items():
                        for col1, v2 in v.items():
                            for col2, data in v2.items():
                                cross_NCC_dict[k][col1][col2].update(data)
                    
                    for k, v in batch_auto_ZNCC.items():
                        for col, data in v.items():
                            auto_ZNCC_dict[k][col].update(data)
                    
                    for k, v in batch_cross_ZNCC.items():
                        for col1, v2 in v.items():
                            for col2, data in v2.items():
                                cross_ZNCC_dict[k][col1][col2].update(data)
                    
                    if calc_AMI and batch_auto_AMI:
                        for k, v in batch_auto_AMI.items():
                            for col, data in v.items():
                                auto_AMI_dict[k][col].update(data)
                        
                        for k, v in batch_cross_AMI.items():
                            for col1, v2 in v.items():
                                for col2, data in v2.items():
                                    cross_AMI_dict[k][col1][col2].update(data)
                    
                    if calc_MI and batch_auto_MI:
                        for k, v in batch_auto_MI.items():
                            for col, data in v.items():
                                auto_MI_dict[k][col].update(data)
                        
                        for k, v in batch_cross_MI.items():
                            for col1, v2 in v.items():
                                for col2, data in v2.items():
                                    cross_MI_dict[k][col1][col2].update(data)
                    
                except Exception as exc:
                    print(f'Batch {batch_idx} generated an exception: {exc}')
    
    # The rest of the function (DataFrame construction) remains the same as original
    # ... (DataFrame construction code from original function) ...
    
    return (df_auto_NCC, df_cross_NCC, df_auto_ZNCC, df_cross_ZNCC, 
            df_auto_AMI, df_cross_AMI, df_auto_MI, df_cross_MI)



